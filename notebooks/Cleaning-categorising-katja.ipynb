{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as mgn\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data with standard preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the new file only - for now\n",
    "df1 = pd.read_excel('../data/data_d-drivers_2024-03-24.xlsx', sheet_name='data',\n",
    "                    )\n",
    "df2 = pd.read_excel('../data/data_d-drivers_2024-03-26.xlsx', sheet_name='data')\n",
    "\n",
    "df1.columns = [col.lower() for col in df1.columns]\n",
    "df2.columns = [col.lower() for col in df2.columns]\n",
    "\n",
    "df1.rename({\n",
    "           'impressions': 'page_impressions',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'published_at': 'publishing_date',\n",
    "           'page_canonical_url': 'url',\n",
    "           'page_author': 'authors', \n",
    "            }, axis=1, inplace=True)\n",
    "\n",
    "df2.rename({\n",
    "           'impressions': 'page_impressions',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'published_at': 'publishing_date',\n",
    "           'page_canonical_url': 'url',\n",
    "           'page_author': 'authors', \n",
    "            }, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes each entry unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[['page_id', 'date', 'url', 'authors', 'word_count']].duplicated(keep=False)] # keep=False keeps all duplicated values\n",
    "                                    #.sort_values(['page_id', 'date', 'url', 'page_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By experimenting with different sets of columns I found those are \n",
    "\n",
    "> 'page_id', 'date', 'url', 'authors', 'word_count'\n",
    "\n",
    "* NOT the `page_name`: it is totally broken\n",
    "* NOT the `publishing_date`: sometimes the articles changed several times during the day and the word count changed, so `publishing_date` does not capture all combinations\n",
    "\n",
    "Addressing the rows by all of those columns makes every article unique but just a single one: 1018299 (rows 66544 and 78658). But the second entry is just a mistake with all missing values, it can be simply dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(78658, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge on those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_columns = ['page_id', 'date', 'url', 'authors', 'word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What columns are in `df1` and not in `df2`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.columns.difference(df1.columns))\n",
    "print(df1.columns.difference(df2.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the page impressions only present in the 2nd delivery, and clockouts only in the first delivery. \n",
    "We will merge the *first to the second*, so in the first dataset we only want to have the key columns and the unique one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[key_columns + ['clickouts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What pages are in df1 and not in df2 and the other way around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.set_index('page_id').index.difference(df2.set_index('page_id').index))\n",
    "print(df2.set_index('page_id').index.difference(df1.set_index('page_id').index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The new data delivery includes all pages from the first one + 8 new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `URLs` are in df1 and not in df2 and the other way around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.set_index('url').index.difference(df2.set_index('url').index))\n",
    "print(df2.set_index('url').index.difference(df1.set_index('url').index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 22 new URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which dates are new?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.set_index('date').index.difference(df2.set_index('date').index))\n",
    "print(df2.set_index('date').index.difference(df1.set_index('date').index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The day between the first and the second data deliveries, makes a lot of sense :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df1.shape == df1.drop_duplicates().shape:\n",
    "    print('No duplicates left in the first dataframe')\n",
    "else: \n",
    "    print('Duplicated entries present: merging will blow up the data frame size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df2[key_columns].shape == df2[key_columns].drop_duplicates().shape:\n",
    "    print('No duplicates are in the second dataframe')\n",
    "else: \n",
    "    print('Duplicated entries present: merging will blow up the data frame size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2[key_columns].duplicated(keep=False)] # keep=False keeps all duplicated values\n",
    "                                    #.sort_values(['page_id', 'date', 'url', 'page_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same mistake is in `df2`: remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(40600, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df2[key_columns].shape == df2[key_columns].drop_duplicates().shape:\n",
    "    print('No duplicates are in the second dataframe')\n",
    "else: \n",
    "    print('Duplicated entries present: merging will blow up the data frame size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuppi, now we are ready to merge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "Using the `left` merging: we already know that `df1` is malformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(left=df2, right=df1, on=key_columns, how='left') \n",
    "# in principle, even the page_id is redundant in this case, \n",
    "# because each url contains the page id as the suffix\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1\n",
    "del df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['page_id', 'date', 'publishing_date', 'url']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['publish_date_equal_to_date'] == 'Y') & (df['publishing_date'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: some articles were scheduled for an update (or first publication), therefore have already existing entries and stats (likes, dislikes, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_articles = df.query('date < publishing_date')#[df.columns.drop(['url', 'title', 'page_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_articles_ids = scheduled_articles.page_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping, so we are not filling in the values from other articles!\n",
    "versions = df.groupby(['page_id'], as_index=False, sort=True)[\n",
    "    ['page_id', 'date', 'publishing_date', 'word_count']\n",
    "    ].ffill()\n",
    "#versions.rename({'index': 'page_id'}, axis=1, inplace=True)\n",
    "versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions.query('date < publishing_date').shape[0] == scheduled_articles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf_idx = versions.query('date < publishing_date').index.difference(scheduled_articles.index)\n",
    "wtf_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> We'll assume that all articles which don't have a publishing date but were scheduled for an update are actually REALLY OLD and were published at the around inception of EFAHRER (01-01-2018) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.date < df.publishing_date, 'publishing_date'] = pd.Timestamp('2018-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some pages we simply don't know the date of first publication! We need to impute it for modelling.\n",
    "Which articles are those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_publ_date = versions[versions.publishing_date.isna()].page_id.unique()\n",
    "no_publ_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_publ_date.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_publ_date) / len(df.page_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those entries comprise almost 34 % of all articles :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For those we apply the same imputing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.page_id.isin(no_publ_date), 'publishing_date'] = pd.Timestamp('2018-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Assuming that when the word counts do not change unless ptherwise specified </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions2 = df.groupby(['page_id'], as_index=False, sort=True)[\n",
    "    ['page_id', 'date', 'publishing_date', 'word_count']\n",
    "    ].ffill().drop_duplicates()\n",
    "\n",
    "versions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions[(versions.date < versions.publishing_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the imputed columns back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.drop('publishing_date').drop('word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = pd.merge(df[df.columns.drop('publishing_date').drop('word_count')], # drop the non-imputed columns\n",
    "                      versions2,\n",
    "                      on=['page_id', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daily_likes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging in the scraped data\n",
    "\n",
    "Thanks Clara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = pd.read_csv('../data/scraping_no_duplicates.csv')\n",
    "df_scraped.columns = [col.lower() for col in df_scraped.columns]\n",
    "\n",
    "df_scraped.rename({\n",
    "           #'impressions': 'page_impressions',\n",
    "           'words': 'words_scraped',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'page_canonical_url': 'url',\n",
    "           'author': 'author_scraped',\n",
    "           'current_title': 'h1'\n",
    "            }, axis=1, inplace=True)\n",
    "\n",
    "col_to_merge = ['page_id', 'url']\n",
    "df_full = pd.merge(left=df, right=df_scraped, on=col_to_merge, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.drop('publish_date_equal_to_date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df_full);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.sort_values(['page_id', 'date', 'publishing_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Imputing with forward-fill </font>\n",
    "<font color=red> Impute word counts with 0 </font> \n",
    "\n",
    "-> In the future: take the value of the `word count (scraped)` or the mean value for the given category! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['word_count'] = df_full.groupby(['page_id', 'date'])['word_count'].ffill()\n",
    "df_full['word_count'] = df_full['word_count'].fillna(0)\n",
    "\n",
    "df_full['publishing_date'] = df_full.groupby(['page_id', 'date'])['publishing_date'].ffill()\n",
    "df_full['publishing_date'] = df_full.groupby(['page_id'])['publishing_date'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating \n",
    "\n",
    "For using in subprojects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_unique_features = ['page_id', 'publishing_date', 'word_count', 'authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_full[page_unique_features + ['date']].drop_duplicates().value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this means they are really unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_versions['version_n'] = \n",
    "temp = df_full[['page_id', 'word_count', 'publishing_date', 'authors']].drop_duplicates()\n",
    "temp = temp.fillna({'word_count': 0, 'publishing_date': pd.Timestamp('2018-01-01 00:00')})\n",
    "temp = temp.drop_duplicates()\n",
    "temp = temp.sort_values('publishing_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_versions = temp.groupby('page_id')['word_count'].transform(lambda x: pd.factorize(x)[0])\n",
    "publish_versions = temp.groupby('page_id')['publishing_date'].transform(lambda x: pd.factorize(x)[0])\n",
    "authors_versions = temp.groupby('page_id')['authors'].transform(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_count = 1000*wc_versions + 100*publish_versions + 1*authors_versions\n",
    "temp['ver_id_wc'] = wc_versions\n",
    "temp['ver_id_pub'] = publish_versions\n",
    "temp['ver_id_auth'] = authors_versions\n",
    "temp['version_id_raw'] = version_count\n",
    "#version_count = pd.factorize(version_count)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['version_id'] = temp.groupby('page_id')['version_id_raw'].transform(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_versions = pd.merge(df_full, temp.drop(['ver_id_wc', 'ver_id_pub', 'ver_id_auth', 'version_id_raw'], axis=1),\n",
    "         on=['page_id', 'word_count', 'publishing_date', 'authors'],\n",
    "         how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full_versions['word_count'] = df_full_versions.groupby('page_id')['word_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User side features: those which the reader sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_side_features = ['page_id', 'date', 'publish_date', 'word_count', 'words_scraped', \n",
    "                    'page_title', 'page_name', 'title', 'h1', 'authors',  \n",
    "                    'classification_product', 'classification_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article (content) versions\n",
    "\n",
    "We want to label each version.\n",
    "* Version changes when there is a new `publication date`\n",
    "* Version changes when there is a new `word count`\n",
    "* Version does NOT change with a change in `URL`\n",
    "* Version does NOT change with a change in the `date` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the necessary columns\n",
    "# we still need the 'date' column for imputation\n",
    "df_cnt = df[['page_efahrer_id', 'date', 'published_at', 'word_count']]\n",
    "df_cnt = df_cnt.sort_values(['page_efahrer_id', 'date', 'published_at'])\n",
    "df_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df_cnt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns where the publication date changed but the `word count` was not updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcna_idx = df_cnt[df_cnt.word_count.isna() & df_cnt.published_at.notna()].index\n",
    "wcna_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT the other way around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt[df_cnt.word_count.notna() & df_cnt.published_at.isna()].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best assumption that it did not change (significantly??), so still forward-fill it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt = df_cnt.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus = df_cnt.loc[wcna_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt['publ_at_enc'] = df_cnt.groupby('page_efahrer_id')['published_at'].transform(lambda x: pd.factorize(x)[0])\n",
    "df_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many versions does each article have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = df_cnt[['page_efahrer_id', 'publ_at_enc']].groupby('page_efahrer_id').max()#.reset_index()\n",
    "first_publ_date = df_cnt[['page_efahrer_id', 'published_at']].groupby('page_efahrer_id').min()\n",
    "first_publ_date = first_publ_date.rename({'published_at': 'First publication date'}, axis=1)\n",
    "to_plot = to_plot.join(first_publ_date)\n",
    "to_plot = to_plot.rename({'publ_at_enc': 'Number of versions'}, axis=1)\n",
    "to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=to_plot, x='Number of versions', y='First publication date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article with 61 (!!!) versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_cols = ['page_canonical_url', 'daily_likes', \n",
    "               'daily_dislikes', 'impressions', 'video_play', \n",
    "               'discover_clicks', 'discover_impressions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article105259 = df[df['page_efahrer_id']==105259].sort_values(['date', 'page_canonical_url'])\n",
    "article_first_url = article105259[metrics_cols + ['date']].drop_duplicates(subset=['date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xticks=pd.date_range(df.date.min(), df.date.max(), freq='2M')\n",
    "fig = article_first_url.plot(kind='bar', x='date', y=metrics_cols, subplots=True, figsize=(6, 12), \n",
    "                       xticks=xticks)\n",
    "plt.gca().set_xticklabels([x.strftime('%a\\n%d\\n%h\\n%Y') for x in xticks]);\n",
    "#plt.xticks(ticks=df[['impressions', 'published_at']].resample('W', on='published_at').max().index);\n",
    "#plt.xticks(ticks=pd.date_range(df.date.min(), df.date.max(), freq='2M'),\n",
    "#           labels=pd.date_range(df.date.min(), df.date.max(), freq='2M'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('../data/full_data.csv')\n",
    "df_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
