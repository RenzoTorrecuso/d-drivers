{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as mgn\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data with standard preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the new file only - for now\n",
    "df1 = pd.read_excel('../data/data_d-drivers_2024-03-24.xlsx', sheet_name='data',\n",
    "                    )\n",
    "df2 = pd.read_excel('../data/data_d-drivers_2024-03-26.xlsx', sheet_name='data')\n",
    "\n",
    "df1.columns = [col.lower() for col in df1.columns]\n",
    "df2.columns = [col.lower() for col in df2.columns]\n",
    "\n",
    "df1.rename({\n",
    "           'impressions': 'page_impressions',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'published_at': 'publishing_date',\n",
    "           'page_canonical_url': 'url',\n",
    "           'page_author': 'authors', \n",
    "            }, axis=1, inplace=True)\n",
    "\n",
    "df2.rename({\n",
    "           'impressions': 'page_impressions',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'published_at': 'publishing_date',\n",
    "           'page_canonical_url': 'url',\n",
    "           'page_author': 'authors', \n",
    "            }, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes each entry unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[['page_id', 'date', 'url', 'authors', 'word_count']].duplicated(keep=False)] # keep=False keeps all duplicated values\n",
    "                                    #.sort_values(['page_id', 'date', 'url', 'page_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By experimenting with different sets of columns I found those are \n",
    "\n",
    "> 'page_id', 'date', 'url', 'authors', 'word_count'\n",
    "\n",
    "* NOT the `page_name`: it is totally broken\n",
    "* NOT the `publishing_date`: sometimes the articles changed several times during the day and the word count changed, so `publishing_date` does not capture all combinations\n",
    "\n",
    "Addressing the rows by all of those columns makes every article unique but just a single one: 1018299 (rows 66544 and 78658). But the second entry is just a mistake with all missing values, it can be simply dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(78658, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge on those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_columns = ['page_id', 'date', 'url', 'authors', 'word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What columns are in `df1` and not in `df2`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.columns.difference(df1.columns))\n",
    "print(df1.columns.difference(df2.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the page impressions only present in the 2nd delivery, and clockouts only in the first delivery. \n",
    "We will merge the *first to the second*, so in the first dataset we only want to have the key columns and the unique one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[key_columns + ['clickouts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What pages are in df1 and not in df2 and the other way around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.set_index('page_id').index.difference(df2.set_index('page_id').index))\n",
    "print(df2.set_index('page_id').index.difference(df1.set_index('page_id').index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The new data delivery includes all pages from the first one + 8 new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `URLs` are in df1 and not in df2 and the other way around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.set_index('url').index.difference(df2.set_index('url').index))\n",
    "print(df2.set_index('url').index.difference(df1.set_index('url').index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 22 new URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which dates are new?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.set_index('date').index.difference(df2.set_index('date').index))\n",
    "print(df2.set_index('date').index.difference(df1.set_index('date').index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The day between the first and the second data deliveries, makes a lot of sense :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df1.shape == df1.drop_duplicates().shape:\n",
    "    print('No duplicates left in the first dataframe')\n",
    "else: \n",
    "    print('Duplicated entries present: merging will blow up the data frame size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df2[key_columns].shape == df2[key_columns].drop_duplicates().shape:\n",
    "    print('No duplicates are in the second dataframe')\n",
    "else: \n",
    "    print('Duplicated entries present: merging will blow up the data frame size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2[key_columns].duplicated(keep=False)] # keep=False keeps all duplicated values\n",
    "                                    #.sort_values(['page_id', 'date', 'url', 'page_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same mistake is in `df2`: remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(40600, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df2[key_columns].shape == df2[key_columns].drop_duplicates().shape:\n",
    "    print('No duplicates are in the second dataframe')\n",
    "else: \n",
    "    print('Duplicated entries present: merging will blow up the data frame size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuppi, now we are ready to merge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "Using the `left` merging: we already know that `df1` is malformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(left=df2, right=df1, on=key_columns, how='left') \n",
    "# in principle, even the page_id is redundant in this case, \n",
    "# because each url contains the page id as the suffix\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging in the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article (content) versions\n",
    "\n",
    "We want to label each version.\n",
    "* Version changes when there is a new `publication date`\n",
    "* Version changes when there is a new `word count`\n",
    "* Version does NOT change with a change in `URL`\n",
    "* Version does NOT change with a change in the `date` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the necessary columns\n",
    "# we still need the 'date' column for imputation\n",
    "df_cnt = df[['page_efahrer_id', 'date', 'published_at', 'word_count']]\n",
    "df_cnt = df_cnt.sort_values(['page_efahrer_id', 'date', 'published_at'])\n",
    "df_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df_cnt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns where the publication date changed but the `word count` was not updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcna_idx = df_cnt[df_cnt.word_count.isna() & df_cnt.published_at.notna()].index\n",
    "wcna_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT the other way around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt[df_cnt.word_count.notna() & df_cnt.published_at.isna()].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best assumption that it did not change (significantly??), so still forward-fill it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt = df_cnt.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus = df_cnt.loc[wcna_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt['publ_at_enc'] = df_cnt.groupby('page_efahrer_id')['published_at'].transform(lambda x: pd.factorize(x)[0])\n",
    "df_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many versions does each article have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = df_cnt[['page_efahrer_id', 'publ_at_enc']].groupby('page_efahrer_id').max()#.reset_index()\n",
    "first_publ_date = df_cnt[['page_efahrer_id', 'published_at']].groupby('page_efahrer_id').min()\n",
    "first_publ_date = first_publ_date.rename({'published_at': 'First publication date'}, axis=1)\n",
    "to_plot = to_plot.join(first_publ_date)\n",
    "to_plot = to_plot.rename({'publ_at_enc': 'Number of versions'}, axis=1)\n",
    "to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=to_plot, x='Number of versions', y='First publication date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article with 61 (!!!) versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_cols = ['page_canonical_url', 'daily_likes', \n",
    "               'daily_dislikes', 'impressions', 'video_play', \n",
    "               'discover_clicks', 'discover_impressions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article105259 = df[df['page_efahrer_id']==105259].sort_values(['date', 'page_canonical_url'])\n",
    "article_first_url = article105259[metrics_cols + ['date']].drop_duplicates(subset=['date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xticks=pd.date_range(df.date.min(), df.date.max(), freq='2M')\n",
    "fig = article_first_url.plot(kind='bar', x='date', y=metrics_cols, subplots=True, figsize=(6, 12), \n",
    "                       xticks=xticks)\n",
    "plt.gca().set_xticklabels([x.strftime('%a\\n%d\\n%h\\n%Y') for x in xticks]);\n",
    "#plt.xticks(ticks=df[['impressions', 'published_at']].resample('W', on='published_at').max().index);\n",
    "#plt.xticks(ticks=pd.date_range(df.date.min(), df.date.max(), freq='2M'),\n",
    "#           labels=pd.date_range(df.date.min(), df.date.max(), freq='2M'));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
