{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== This is the script that combines and tidies up the raw data ========\n",
      "The run should take approx. 30 seconds.\n",
      "\n",
      "Reading the first file...\n",
      "Reading the second file...\n",
      "Reading complete. \n",
      "Cleaning up the dataframes...\n",
      "Merging...\n",
      "Imputing...\n"
     ]
    }
   ],
   "source": [
    "## For explanations see ./notebooks/Cleaning-categorising-katja.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "### from timeit import timeit\n",
    "\n",
    "# Read part of the malformatted file:\n",
    "print('======== This is the script that combines and tidies up the raw data ========')\n",
    "print('The run should take approx. 30 seconds.')\n",
    "print()\n",
    "print('Reading the first file...')\n",
    "\n",
    "df1 = pd.read_excel('../data/data_d-drivers_2024-03-24.xlsx', sheet_name='data',\n",
    "                    usecols=['PAGE_EFAHRER_ID', 'DATE', 'PAGE_CANONICAL_URL', 'PAGE_AUTHOR', 'WORD_COUNT', 'CLICKOUTS']\n",
    "                    )\n",
    "print('Reading the second file...')\n",
    "#Read everything from the new file:\n",
    "df2 = pd.read_excel('../data/data_d-drivers_2024-03-26.xlsx', sheet_name='data')\n",
    "\n",
    "print('Reading complete. \\nCleaning up the dataframes...')\n",
    "df1.columns = [col.lower() for col in df1.columns]\n",
    "df2.columns = [col.lower() for col in df2.columns]\n",
    "\n",
    "df1.rename({\n",
    "           #'impressions': 'page_impressions',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'published_at': 'publish_date',\n",
    "           'page_canonical_url': 'url',\n",
    "           'page_author': 'authors', \n",
    "            }, axis=1, inplace=True)\n",
    "\n",
    "df2.rename({\n",
    "           'impressions': 'page_impressions',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'published_at': 'publish_date',\n",
    "           'page_canonical_url': 'url',\n",
    "           'page_author': 'authors', \n",
    "            }, axis=1, inplace=True)\n",
    "\n",
    "# Eliminate mistakes from the table\n",
    "df1.drop(78658, inplace=True)\n",
    "df2.drop(40600, inplace=True)\n",
    "\n",
    "### Merging ###\n",
    "\n",
    "# Using the `left` merging: we already know that `df1` is malformatted\n",
    "key_columns = ['page_id', 'date', 'url', 'authors', 'word_count']\n",
    "\n",
    "print('Merging...')\n",
    "df = pd.merge(left=df2, right=df1, on=key_columns, how='left') \n",
    "\n",
    "### Imputing ###\n",
    "print('Imputing...')\n",
    "df = df.sort_values(['page_id', 'date', 'publish_date', 'url'])\\\n",
    "    .reset_index(drop=False)\n",
    "df.rename({'index': 'old_index'}, axis=1, inplace=True)\n",
    "\n",
    "# reshuffling columns\n",
    "df = df[['old_index', 'page_id', 'date', 'publish_date', 'word_count',\n",
    "       'publish_date_equal_to_date', # we don't need this one anymore (and never needed?)\n",
    "       'url', 'page_name', 'classification_product', 'classification_type',\n",
    "       'title', 'authors', 'daily_likes', 'daily_dislikes', \n",
    "       'video_play', 'page_impressions', 'clickouts', \n",
    "       'external_clicks', 'external_impressions'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputing ###\n",
    "# Filling in missing publishing dates: \n",
    "# First, we assume that when the current `date` precedes the publish_date\n",
    "# the article was initially published long ago, where \"long ago\" is 1. Jan 2018\n",
    "# and today it has been scheduled for an update\n",
    "\n",
    "df_imputed = df.copy()## Placeholder\n",
    "\n",
    "###### df_imputed.loc[df.date < df.publish_date, 'publish_date'] = pd.Timestamp('2018-01-01 00:00:00')\n",
    "\n",
    "# second, we assume that whern there is date of publication at all, \n",
    "# the articles were published on 1. Jan 2018 (Roughly 33% of all articles)\n",
    "\n",
    "# versions_ = df_imputed.groupby(['page_id'], as_index=False, sort=True)[\n",
    "#     ['page_id', 'date', 'publish_date', 'word_count']\n",
    "#     ].ffill()\n",
    "# # which articles don't have the publishing date?\n",
    "\n",
    "###### Broken!!!\n",
    "# no_publ_date = versions_[versions_.publish_date.isna()].page_id.unique()\n",
    "\n",
    "# df_imputed.loc[df.page_id.isin(no_publ_date), 'publish_date'] = pd.Timestamp('2018-01-01 00:00:00')\n",
    "# Forward-filling the word count and publishing dates for each article.\n",
    "# !! Assuming that when the word counts do not change unless otherwise specified !! \n",
    "#versions2 = df_imputed.groupby(['page_id'], as_index=False, sort=True)[\n",
    "#    ['page_id', 'date', 'publish_date', 'word_count']\n",
    "#    ].ffill().drop_duplicates(['page_id', 'date', 'publish_date'])\n",
    "\n",
    "# merging the imputed columns back into the df\n",
    "#df_imputed = pd.merge(df_imputed.drop(['publish_date', 'word_count'], axis=1).drop_duplicates(['page_id', 'date', 'url', 'authors']), # drop the non-imputed columns\n",
    "#                      versions2,\n",
    "#                      on=['page_id', 'date'], how='inner')\n",
    "\n",
    "df_imputed = df_imputed.sort_values(['page_id', 'date', 'publish_date']) # just in case, should be already sorted\n",
    "\n",
    "df_imputed['word_count'] = df_imputed.groupby(['page_id', 'date'])['word_count'].ffill()\n",
    "\n",
    "# Impute the still missing word counts with 0\n",
    "# -> In the future: take the value of the `word count (scraped)` \n",
    "# or the mean value for the given category! \n",
    "df_imputed['word_count'] = df_imputed['word_count'].fillna(0)\n",
    "\n",
    "df_imputed['publish_date'] = df_imputed.groupby(['page_id', 'date'])['publish_date'].ffill()\n",
    "df_imputed['publish_date'] = df_imputed.groupby(['page_id'])['publish_date'].ffill()\n",
    "df_imputed['publish_date'] = df_imputed['publish_date'].fillna(pd.Timestamp('2018-01-01 00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_10437 = df_imputed.query('page_id == 10437')[['page_id', 'date', 'publish_date', 'word_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating version IDs...\n",
      "     ->  Hint: version changes when any of the folowing change: word count, publish_date or the authors\n"
     ]
    }
   ],
   "source": [
    "### Version count ###\n",
    "print('''Calculating version IDs...\n",
    "     ->  Hint: version changes when any of the folowing change: word count, publish_date or the authors''')\n",
    "\n",
    "temp = df_imputed[['page_id', 'word_count', 'publish_date', 'authors']].drop_duplicates().copy()\n",
    "#temp = temp.fillna({'word_count': 0, 'publish_date': pd.Timestamp('2018-01-01 00:00')})\n",
    "temp = temp.drop_duplicates()\n",
    "temp = temp.sort_values('publish_date')\n",
    "\n",
    "wc_versions = temp.groupby('page_id')['word_count'].transform(lambda x: pd.factorize(x)[0])\n",
    "publish_versions = temp.groupby('page_id')['publish_date'].transform(lambda x: pd.factorize(x)[0])\n",
    "authors_versions = temp.groupby('page_id')['authors'].transform(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "version_count = 10000*wc_versions + 1000*publish_versions + 1*authors_versions\n",
    "temp['ver_id_wc'] = wc_versions\n",
    "temp['ver_id_pub'] = publish_versions\n",
    "temp['ver_id_auth'] = authors_versions\n",
    "temp['version_id_raw'] = version_count\n",
    "temp['version_id'] = temp.groupby('page_id')['version_id_raw'].transform(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "df_versions = pd.merge(left=df_imputed, right=temp.drop(['ver_id_wc', 'ver_id_pub', 'ver_id_auth', 'version_id_raw'], axis=1),\n",
    "         on=['page_id', 'word_count', 'publish_date', 'authors'],\n",
    "         how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions = df_versions.sort_values(['page_id', 'date'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_10437 = df_versions.query('page_id == 10437')[['page_id', 'date', 'publish_date', 'word_count', 'version_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>date</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>word_count</th>\n",
       "      <th>version_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>10437</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>10437</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10437</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10437</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>10437</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>10437</td>\n",
       "      <td>2024-02-25</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>10437</td>\n",
       "      <td>2024-02-25</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>10437</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>10437</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>10437</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>2314.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page_id       date publish_date  word_count  version_id\n",
       "65     10437 2023-01-01   2018-01-01         0.0           0\n",
       "66     10437 2023-01-01   2018-01-01         0.0           0\n",
       "67     10437 2023-01-02   2018-01-01         0.0           0\n",
       "68     10437 2023-01-03   2018-01-01         0.0           0\n",
       "69     10437 2023-01-04   2018-01-01         0.0           0\n",
       "..       ...        ...          ...         ...         ...\n",
       "314    10437 2024-02-25   2024-02-19      2314.0          29\n",
       "315    10437 2024-02-25   2024-02-19      2314.0          29\n",
       "316    10437 2024-02-26   2024-02-19      2314.0          29\n",
       "317    10437 2024-02-26   2024-02-19      2314.0          29\n",
       "318    10437 2024-02-26   2024-02-19      2314.0          29\n",
       "\n",
       "[254 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_10437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Including the scraped data ###\n",
    "# THANKS @CLARA \n",
    "print('Merging with the scraped data...')\n",
    "\n",
    "df_scraped = pd.read_csv('../data/scraping_no_duplicates.csv')\n",
    "df_scraped.columns = [col.lower() for col in df_scraped.columns]\n",
    "\n",
    "df_scraped.rename({\n",
    "           #'impressions': 'page_impressions',\n",
    "           'words': 'words_scraped',\n",
    "           'page_efahrer_id': 'page_id',\n",
    "           'page_canonical_url': 'url',\n",
    "           'author': 'author_scraped',\n",
    "           'current_title': 'h1'\n",
    "            }, axis=1, inplace=True)\n",
    "\n",
    "merge_keys = ['page_id', 'url']\n",
    "df_full = pd.merge(left=df_versions, right=df_scraped, on=merge_keys, how='left')\n",
    "# May drop some columns\n",
    "#df_full = df_full.drop(['old_index'], axis=1)\n",
    "\n",
    "### Final touch\n",
    "df_full = df_full[['old_index', 'page_id', 'date', 'url', 'version_id', 'publish_date',\n",
    "       'word_count', 'words_scraped', 'classification_product', 'classification_type', \n",
    "       'page_name', 'authors', 'author_scraped',\n",
    "       'title', 'h1', 'abstract', 'last_update', 'image_url',\n",
    "       'daily_likes', 'daily_dislikes', 'video_play', 'page_impressions',\n",
    "       'clickouts', 'external_clicks', 'external_impressions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_10437 = df_full.query('page_id == 10437')[['page_id', 'date', 'publish_date', 'word_count', 'version_id']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
