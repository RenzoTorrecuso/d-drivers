{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'aiplatform' from 'google.cloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aiplatform\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport_model_sample\u001b[39m(\n\u001b[1;32m      4\u001b[0m     project: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m     model_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# The AI Platform services require regional API endpoints.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     client_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: api_endpoint}\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'aiplatform' from 'google.cloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "def export_model_sample(\n",
    "    project: str,\n",
    "    model_id: str,\n",
    "    gcs_destination_output_uri_prefix: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    timeout: int = 300,\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    output_config = {\n",
    "        \"artifact_destination\": {\n",
    "            \"output_uri_prefix\": gcs_destination_output_uri_prefix\n",
    "        },\n",
    "        # For information about export formats: https://cloud.google.com/ai-platform-unified/docs/export/export-edge-model#aiplatform_export_model_sample-drest\n",
    "        \"export_format_id\": \"tf-saved-model\",\n",
    "    }\n",
    "    name = client.model_path(project=project, location=location, model=model_id)\n",
    "    response = client.export_model(name=name, output_config=output_config)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    print(\"output_info:\", response.metadata.output_info)\n",
    "    export_model_response = response.result(timeout=timeout)\n",
    "    print(\"export_model_response:\", export_model_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai endpoints export \\\n",
    "  --region=europe-north1 \\\n",
    "  --model=transformations-all-features-scaled-target.json\\\n",
    "  --output-uri=gs://cloud-ai-platform-63a5e957-921e-4e42-9b23-8c38a4a83e6a/transformations-all-features-scaled-target.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1988870599.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    gcloud ai endpoints export --region=<REGION> --model=<MODEL_NAME> --output-uri=<OUTPUT_URI>\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gcloud ai endpoints export --region=<REGION> --model=<MODEL_NAME> --output-uri=https://storage.cloud.google.com/cloud-ai-platform-63a5e957-921e-4e42-9b23-8c38a4a83e6a/transformations-all-features-scaled-target.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import load_library\n",
    "#load_library.load_op_library('/path/to/tf_decode_proto_sparse_v4.so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/keras/src/saving/saving_api.py:191\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assets.extra', 'variables', 'saved_model.pb', 'assets']\n",
      "Error loading the model: Op type not registered 'DecodeProtoSparseV4' in binary running on Air-von-Clara.fritz.box. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the path to the saved model\n",
    "model_path = '/Users/clara/Desktop/neuefische/d-drivers/final_model_vertex/tf-saved-model/2024-04-21T13:24:51.408713Z/predict/001/'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(model_path):\n",
    "    # List the files in the directory\n",
    "    files = os.listdir(model_path)\n",
    "    \n",
    "    # Print the files to verify if 'saved_model.pb' is present\n",
    "    print(files)\n",
    "    \n",
    "    # Attempt to load the saved model\n",
    "    try:\n",
    "        model = tf.saved_model.load(model_path)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "else:\n",
    "    print(f\"The directory {model_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Op type not registered 'DecodeProtoSparseV4' in binary running on Air-von-Clara. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:3061\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[0;34m(self, type)\u001b[0m\n",
      "\u001b[1;32m   3060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 3061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DecodeProtoSparseV4'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m graph \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGraph()\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Load the model\u001b[39;00m\n",
      "\u001b[0;32m----> 6\u001b[0m     loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/clara/Desktop/neuefische/d-drivers/tf-saved-model/2024-04-12T07:29:42.410616Z/lower_bound/001\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use the loaded model for inference\u001b[39;00m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# For example:\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# predictions = loaded_model(input_data)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load.py:912\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n",
      "\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n",
      "\u001b[1;32m    911\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n",
      "\u001b[0;32m--> 912\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load.py:1070\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n",
      "\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModels saved from Tensorflow 1.x) cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1068\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded with node filters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m   1069\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n",
      "\u001b[0;32m-> 1070\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mload_v1_in_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_skip_checkpoint\u001b[49m\n",
      "\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1073\u001b[0m     root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m debug_info\n",
      "\u001b[1;32m   1074\u001b[0m \u001b[38;5;66;03m# For privacy concerns, please see the note in\u001b[39;00m\n",
      "\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m#  tensorflow/cc/saved_model/metrics.h\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:309\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, skip_restoring_checkpoint)\u001b[0m\n",
      "\u001b[1;32m    307\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementReadApi(_LOAD_V1_V2_LABEL)\n",
      "\u001b[1;32m    308\u001b[0m loader \u001b[38;5;241m=\u001b[39m _EagerSavedModelLoader(export_dir)\n",
      "\u001b[0;32m--> 309\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_restoring_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_restoring_checkpoint\u001b[49m\n",
      "\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    312\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementRead(write_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:224\u001b[0m, in \u001b[0;36m_EagerSavedModelLoader.load\u001b[0;34m(self, tags, skip_restoring_checkpoint)\u001b[0m\n",
      "\u001b[1;32m    222\u001b[0m meta_graph_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_meta_graph_def_from_tags(tags)\n",
      "\u001b[1;32m    223\u001b[0m load_shared_name_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ops\u001b[38;5;241m.\u001b[39muid())\n",
      "\u001b[0;32m--> 224\u001b[0m functions \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_deserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_function_def_library\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_graph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_shared_name_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_shared_name_suffix\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Replace existing functions in the MetaGraphDef with renamed functions so\u001b[39;00m\n",
      "\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# we don't have duplicates or name collisions.\u001b[39;00m\n",
      "\u001b[1;32m    230\u001b[0m meta_graph_def\u001b[38;5;241m.\u001b[39mgraph_def\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mClear()\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/function_deserialization.py:456\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n",
      "\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# There is no need to copy all functions into the function def graph. It\u001b[39;00m\n",
      "\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# leads to a O(n^2) increase of memory when importing functions and the\u001b[39;00m\n",
      "\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# extra function definitions are a no-op since they already imported as a\u001b[39;00m\n",
      "\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# function before and passed in explicitly (due to the topologic sort\u001b[39;00m\n",
      "\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# import).\u001b[39;00m\n",
      "\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n",
      "\u001b[0;32m--> 456\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_def_to_graph\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    457\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfdef\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_input_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_input_signature\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n",
      "\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# custom gradients)\u001b[39;00m\n",
      "\u001b[1;32m    462\u001b[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/function_def_to_graph.py:91\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes, propagate_device_spec, include_library_functions)\u001b[0m\n",
      "\u001b[1;32m     88\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     89\u001b[0m         input_shapes\u001b[38;5;241m.\u001b[39mappend(input_shape)\n",
      "\u001b[0;32m---> 91\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_to_graph_def\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfdef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_library_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_library_functions\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n",
      "\u001b[1;32m     96\u001b[0m   \u001b[38;5;66;03m# Add all function nodes to the graph.\u001b[39;00m\n",
      "\u001b[1;32m     97\u001b[0m   importer\u001b[38;5;241m.\u001b[39mimport_graph_def_for_function(\n",
      "\u001b[1;32m     98\u001b[0m       graph_def, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, propagate_device_spec\u001b[38;5;241m=\u001b[39mpropagate_device_spec)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/function_def_to_graph.py:330\u001b[0m, in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes, include_library_functions)\u001b[0m\n",
      "\u001b[1;32m    328\u001b[0m       graph_def\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mgradient\u001b[38;5;241m.\u001b[39mextend([grad_def])\n",
      "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 330\u001b[0m   op_def \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_def_for_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m op_def\u001b[38;5;241m.\u001b[39mattr:\n",
      "\u001b[1;32m    333\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:3064\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[0;34m(self, type)\u001b[0m\n",
      "\u001b[1;32m   3061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n",
      "\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;32m   3063\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m op_def_pb2\u001b[38;5;241m.\u001b[39mOpDef\u001b[38;5;241m.\u001b[39mFromString(\n",
      "\u001b[0;32m-> 3064\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_for_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   3065\u001b[0m   )\n",
      "\u001b[1;32m   3066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Op type not registered 'DecodeProtoSparseV4' in binary running on Air-von-Clara. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
     ]
    }
   ],
   "source": [
    "# Create a new TensorFlow graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Load the model\n",
    "    loaded_model = tf.saved_model.load('/Users/clara/Desktop/neuefische/d-drivers/tf-saved-model/2024-04-12T07:29:42.410616Z/lower_bound/001')\n",
    "\n",
    "# Use the loaded model for inference\n",
    "# For example:\n",
    "# predictions = loaded_model(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Op type not registered 'DecodeProtoSparseV4' in binary running on Air-von-Clara. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:3061\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DecodeProtoSparseV4'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m graph \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/clara/Desktop/neuefische/d-drivers/tf-saved-model/2024-04-12T07:29:42.410616Z/lower_bound/001\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use the loaded model for inference\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# For example:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# predictions = loaded_model(input_data)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load.py:912\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[1;32m    911\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[0;32m--> 912\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load.py:1070\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModels saved from Tensorflow 1.x) cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1068\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded with node filters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1069\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m-> 1070\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mload_v1_in_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_skip_checkpoint\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m     root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m debug_info\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;66;03m# For privacy concerns, please see the note in\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m#  tensorflow/cc/saved_model/metrics.h\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:309\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, skip_restoring_checkpoint)\u001b[0m\n\u001b[1;32m    307\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementReadApi(_LOAD_V1_V2_LABEL)\n\u001b[1;32m    308\u001b[0m loader \u001b[38;5;241m=\u001b[39m _EagerSavedModelLoader(export_dir)\n\u001b[0;32m--> 309\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_restoring_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_restoring_checkpoint\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementRead(write_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:224\u001b[0m, in \u001b[0;36m_EagerSavedModelLoader.load\u001b[0;34m(self, tags, skip_restoring_checkpoint)\u001b[0m\n\u001b[1;32m    222\u001b[0m meta_graph_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_meta_graph_def_from_tags(tags)\n\u001b[1;32m    223\u001b[0m load_shared_name_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ops\u001b[38;5;241m.\u001b[39muid())\n\u001b[0;32m--> 224\u001b[0m functions \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_deserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_function_def_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_graph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_shared_name_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_shared_name_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Replace existing functions in the MetaGraphDef with renamed functions so\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# we don't have duplicates or name collisions.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m meta_graph_def\u001b[38;5;241m.\u001b[39mgraph_def\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mClear()\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/function_deserialization.py:456\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# There is no need to copy all functions into the function def graph. It\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# leads to a O(n^2) increase of memory when importing functions and the\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# extra function definitions are a no-op since they already imported as a\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# function before and passed in explicitly (due to the topologic sort\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# import).\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 456\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_def_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfdef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_input_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_input_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# custom gradients)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/function_def_to_graph.py:91\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes, propagate_device_spec, include_library_functions)\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         input_shapes\u001b[38;5;241m.\u001b[39mappend(input_shape)\n\u001b[0;32m---> 91\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_to_graph_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfdef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_library_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_library_functions\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;66;03m# Add all function nodes to the graph.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m   importer\u001b[38;5;241m.\u001b[39mimport_graph_def_for_function(\n\u001b[1;32m     98\u001b[0m       graph_def, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, propagate_device_spec\u001b[38;5;241m=\u001b[39mpropagate_device_spec)\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/function_def_to_graph.py:330\u001b[0m, in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes, include_library_functions)\u001b[0m\n\u001b[1;32m    328\u001b[0m       graph_def\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mgradient\u001b[38;5;241m.\u001b[39mextend([grad_def])\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m   op_def \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_def_for_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m op_def\u001b[38;5;241m.\u001b[39mattr:\n\u001b[1;32m    333\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:3064\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   3063\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m op_def_pb2\u001b[38;5;241m.\u001b[39mOpDef\u001b[38;5;241m.\u001b[39mFromString(\n\u001b[0;32m-> 3064\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_for_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3065\u001b[0m   )\n\u001b[1;32m   3066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Op type not registered 'DecodeProtoSparseV4' in binary running on Air-von-Clara. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
     ]
    }
   ],
   "source": [
    "# Create a new TensorFlow graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Load the model\n",
    "    loaded_model = tf.saved_model.load('/Users/clara/Desktop/neuefische/d-drivers/tf-saved-model/2024-04-12T07:29:42.410616Z/lower_bound/001')\n",
    "\n",
    "# Use the loaded model for inference\n",
    "# For example:\n",
    "# predictions = loaded_model(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = loaded_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from typing import Dict\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import pickle\n",
    "\n",
    "# Initialize PowerTransformer\n",
    "#pt = PowerTransformer()\n",
    "file_path_specific = '/Users/clara/Desktop/neuefische/d-drivers/notebooks/power_transformer_ext_impr.pkl'\n",
    "with open(file_path_specific, 'rb') as file:\n",
    "    loaded_pt = pickle.load(file)\n",
    "\n",
    "# Function to reverse power transformation\n",
    "def reverse_power_transformation(predicted_value, pt):\n",
    "    # Reshape the predicted value for inverse transformation\n",
    "    predicted_value_transformed = predicted_value.reshape(-1, 1)\n",
    "    \n",
    "    # Inverse transform the predicted value\n",
    "    return loaded_pt.inverse_transform(predicted_value_transformed)\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Vertex AI Prediction Demo\")\n",
    "\n",
    "# Text areas for Title, Abstract, and Article\n",
    "title_text = st.text_area(\"Title\", \"Your Title Here\")\n",
    "abstract_text = st.text_area(\"Abstract\", \"Your Abstract Here\")\n",
    "article_text = st.text_area(\"Article\", \"Your Article Here\")\n",
    "\n",
    "# Input features\n",
    "classification_product = st.selectbox(\"Classification Product\", ['E-Auto', 'Auto', 'Zubehör', 'Motorrad', 'Energie', 'Verkehr','Wallbox/Laden', 'Solaranlagen', 'E-Bike', 'Fahrrad', 'E-Scooter','Solarspeicher', 'Balkonkraftwerk', 'Solargenerator', 'THG','Wärmepumpe', 'Versicherung'])\n",
    "classification_type = st.selectbox(\"Classification Type\", ['Ratgeber', 'News', 'Kaufberatung', 'Deal', 'Test','Erfahrungsbericht', 'Video'])\n",
    "urls_per_days = st.select_slider(\"Publishing Frequency\",options=['0','0.25','0.5','0.75','1','1.25','1.5','1.75', '2'])\n",
    "#video_standard_and_widget = st.selectbox(\"Video Standard and Widget\", [\"True\", \"False\"])\n",
    "media_type = st.selectbox(\"Media Type\", [\"Image\",\"Video\"])\n",
    "if media_type == 'Video':\n",
    "    video_widget = st.selectbox(\"Self-produced video\", [\"True\", \"False\"])\n",
    "else:\n",
    "    video_widget = 'False'\n",
    "YOUR_N_DAYS_VALUE = st.number_input(\"Expected time the article is online\", 100)\n",
    "\n",
    "# Count characters in Title, Abstract, and Article\n",
    "h1_len = len(title_text)\n",
    "meta_title_len = len(title_text)\n",
    "abstract_len = len(abstract_text) \n",
    "meta_desc_len = abstract_len - 100\n",
    "word_count = abstract_len+ len(article_text)\n",
    "\n",
    "# Prepare instance dictionary\n",
    "instance_dict = {\n",
    "    \"word_count\": str(word_count),\n",
    "    \"classification_product\": classification_product,\n",
    "    \"classification_type\": classification_type,\n",
    "    \"urls_per_days\": urls_per_days,\n",
    "    \"meta_title_len\": str(meta_title_len),\n",
    "    \"meta_desc_len\": str(meta_desc_len),\n",
    "    \"h1_len\": str(h1_len),\n",
    "    \"abstract_len\": str(abstract_len),\n",
    "    \"google_trend_label\": str('elektroauto'),\n",
    "    \"google_trend_score\": str(31.0),\n",
    "    \"video_standard_and_widget\": str('False') if video_widget == 'True' else str('True'),\n",
    "    \"video_widget\": str('True') if video_widget == 'True' else str('False'),\n",
    "    \"media_type_other\": str('False'),\n",
    "    \"media_type_video\": str('True') if media_type == 'Video' else str('False'),\n",
    "    \"Authors\": str(\"lemur\")\n",
    "}\n",
    "\n",
    "\n",
    "def predict_tabular_regression_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instance_dict: Dict,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    \n",
    "    # Parse the instance dictionary\n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    \n",
    "    # Prepare empty parameters\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    \n",
    "    # Construct the endpoint path\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Send the prediction request\n",
    "        response = client.predict(\n",
    "            endpoint=endpoint, instances=instances, parameters=parameters\n",
    "        )\n",
    "\n",
    "        # Display the response\n",
    "        st.write(\"Response:\")\n",
    "        st.write(f\"Deployed Model ID: {response.deployed_model_id}\")\n",
    "        \n",
    "        # Print the predictions\n",
    "        predictions = response.predictions\n",
    "        for prediction in predictions:\n",
    "            st.write(\"Prediction:\", dict(prediction))\n",
    "            \n",
    "    except Exception as e:\n",
    "        st.write(f\"Error during prediction: {e}\")\n",
    "        st.write(f\"Error type: {type(e)}\")\n",
    "\n",
    "\n",
    "# Button to trigger prediction\n",
    "if st.button(\"Predict\"):\n",
    "    response = predict_tabular_regression_sample(\n",
    "        project=\"101568381799\",\n",
    "        endpoint_id=\"5222247024354656256\",\n",
    "        instance_dict=instance_dict,\n",
    "        location=\"us-central1\",\n",
    "        api_endpoint=\"us-central1-aiplatform.googleapis.com\"\n",
    "        )\n",
    "\n",
    "    # Extract prediction value from the response\n",
    "    prediction_value = response.predictions[0]  # Assuming you're interested in the first prediction\n",
    "    st.write(f\"Predicted Value: {prediction_value}\")\n",
    "\n",
    "    # Reverse Power Transformation\n",
    "    reversed_value_transformed = reverse_power_transformation(predicted_value, loaded_pt)\n",
    "    \n",
    "    # Reverse Normalization by multiplying through n_days\n",
    "    n_days = YOUR_N_DAYS_VALUE \n",
    "    reversed_value = reversed_value_transformed * n_days\n",
    "    \n",
    "    # Display the reversed value\n",
    "    st.write(f\"Predicted Value (Original Scale): {reversed_value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'RegressionExperiment' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[33], line 47\u001b[0m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pf \u001b[38;5;129;01min\u001b[39;00m publ_freq_range:\n",
      "\u001b[1;32m     46\u001b[0m     inp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murls_per_days\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pf\n",
      "\u001b[0;32m---> 47\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     48\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Assuming the predicted value is stored in the 'Label' column\u001b[39;00m\n",
      "\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Now, predictions contains the predicted values for the range of publishing frequencies\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pycaret/regression/functional.py:1895\u001b[0m, in \u001b[0;36mpredict_model\u001b[0;34m(estimator, data, round, verbose)\u001b[0m\n",
      "\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experiment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   1893\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m _EXPERIMENT_CLASS()\n",
      "\u001b[0;32m-> 1895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pycaret/regression/oop.py:2188\u001b[0m, in \u001b[0;36mRegressionExperiment.predict_model\u001b[0;34m(self, estimator, data, round, verbose)\u001b[0m\n",
      "\u001b[1;32m   2134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_model\u001b[39m(\n",
      "\u001b[1;32m   2135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m   2136\u001b[0m     estimator,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   2139\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m   2140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n",
      "\u001b[1;32m   2141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   2142\u001b[0m \u001b[38;5;124;03m    This function predicts ``Label`` using a trained model. When ``data`` is\u001b[39;00m\n",
      "\u001b[1;32m   2143\u001b[0m \u001b[38;5;124;03m    None, it predicts label on the holdout set.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   2185\u001b[0m \n",
      "\u001b[1;32m   2186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 2188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   2189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoded_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:4992\u001b[0m, in \u001b[0;36m_SupervisedExperiment.predict_model\u001b[0;34m(self, estimator, data, probability_threshold, encoded_labels, raw_score, round, verbose, ml_usecase, preprocess)\u001b[0m\n",
      "\u001b[1;32m   4989\u001b[0m         probability_threshold \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mprobability_threshold\n",
      "\u001b[1;32m   4990\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m get_estimator_from_meta_estimator(estimator)\n",
      "\u001b[0;32m-> 4992\u001b[0m pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_test_))\n",
      "\u001b[1;32m   4993\u001b[0m pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39minverse_transform(pred)\n",
      "\u001b[1;32m   4994\u001b[0m \u001b[38;5;66;03m# Need to convert labels back to numbers\u001b[39;00m\n",
      "\u001b[1;32m   4995\u001b[0m \u001b[38;5;66;03m# TODO optimize\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RegressionExperiment' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycaret.regression import *\n",
    "\n",
    "# Assuming you have set up your data and trained the model as shown in your code\n",
    "# num_bl and cat_bl contain the numeric and categorical feature names, respectively\n",
    "# target contains the target variable name\n",
    "\n",
    "# Define the input vector\n",
    "inp_vector = {\n",
    "    'word_count': [1000],\n",
    "    'urls_per_days': [0.25],\n",
    "    'meta_title_len': [60],\n",
    "    'meta_desc_len': [150],\n",
    "    'h1_len': [40],\n",
    "    'abstract_len': [200],\n",
    "    'google_trend_score': [31.0],\n",
    "    'Authors': ['lemur'],\n",
    "    'media_type_video': ['True'],\n",
    "    'video_widget': ['True'],\n",
    "    'video_standard_and_widget': ['False'],\n",
    "    'google_trend_label': ['elektroauto'],\n",
    "    'classification_type': ['News'],\n",
    "    'classification_product': ['E-Auto']\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "inp_df = pd.DataFrame.from_dict(inp_vector)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "inp_df = pd.get_dummies(inp_df, columns=cat_bl, drop_first=True)\n",
    "\n",
    "# Ensure all columns in cat_bl are present in inp_df after one-hot encoding\n",
    "missing_columns = set(cat_bl) - set(inp_df.columns)\n",
    "for col in missing_columns:\n",
    "    inp_df[col] = 0\n",
    "\n",
    "# Define the range of publishing frequencies\n",
    "publ_freq_range = np.linspace(0.001, 0.45, 100)\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Loop through the range of publishing frequencies and make predictions\n",
    "for pf in publ_freq_range:\n",
    "    inp_df['urls_per_days'] = pf\n",
    "    pred = predict_model(s, data=inp_df)\n",
    "    predictions.append(pred['Label'][0])  # Assuming the predicted value is stored in the 'Label' column\n",
    "\n",
    "# Now, predictions contains the predicted values for the range of publishing frequencies\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
