{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for labeling Articles with ranking of related google searches (extracted from google trends)\n",
    "\n",
    "\n",
    "\n",
    "**Preface:** The classification process can be accelerated by using GPU Power. If you use Google Collab with a T4 runtime environment, you can speed up the processing time by a factor of 10 compared to a Mac 2020 with a 6-core Intel CPU. \n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here for considerations on google colab…\n",
    "</summary>\n",
    "\n",
    "**1. loading the data**\n",
    "\n",
    "Upload your source files to your google drive and replace the code in section \"## 1. Load data for labeling process\" with the following code\n",
    "\n",
    ">```python\n",
    ">from google.colab import drive\n",
    ">drive.mount(\"/content/drive\")\n",
    ">\n",
    ">file_path_features = 'drive/My Drive/Colab Notebooks/data_features.csv'\n",
    ">file_path_labels = 'drive/My Drive/Colab Notebooks/related_queries.csv'\n",
    ">\n",
    ">df = pd.read_csv(file_path_features)\n",
    ">df_labels = pd.read_csv(file_path_labels)\n",
    ">```\n",
    "\n",
    "**2. loading the model**\n",
    "\n",
    "When google colab tries to create the pipeline with a model from huggingface.co it likely will complain `OSError: You are trying to access a gated repo.`. \n",
    "You still can access it by adding your api_token from huggingface. [How to create a user access token for huggingface](https://huggingface.co/docs/hub/en/security-tokens). \n",
    "You can add your user access token to your script for loading the moddel from huggingface. Once you loaded the model for the first time you should remove the token from the notebook before you save it. As long as the colab notebook is connected to the runtime environment the model will be available. If you loose the connection for some reason you need your token again.\n",
    "\n",
    "Replace the codeblock \"# Initialize the pipeline\" with the following code if you want to use google colab:\n",
    "\n",
    ">```python\n",
    ">import os\n",
    ">api_token = \"...\" # remove token after successful initializing the pipeline\n",
    ">os.environ[\"HF_TOKEN\"] = api_token # environment variable has to be set additionally to the parameter of the pipeline function\n",
    ">classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\", use_auth_token=api_token, device=0)\n",
    ">```\n",
    "\n",
    "</details>\n",
    "\n",
    "**Main Chapters:**\n",
    "\n",
    "**1.** Load data for labeling process\n",
    "\n",
    "**2.** Enrich page_ids with google score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:03:52.201811: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data for labeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_features = '../data/data_features.csv'\n",
    "file_path_labels = '../data/related_queries.csv'\n",
    "\n",
    "df = pd.read_csv(file_path_features)\n",
    "df_labels = pd.read_csv(file_path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6815, 54)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enrich page_ids with google score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Functions for selecting score with highest probability and looping over dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_score(prediction):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Function for returning the labels and scores with the highes prediction probability\n",
    "        from a given classification pipeline\n",
    "    Args:\n",
    "        prediction (pipeline): classification pipeline\n",
    "\n",
    "    Returns:\n",
    "        max_labe (str): label with highest probability\n",
    "        max_probability (float): highest probability\n",
    "    \"\"\"\n",
    "    pred_labels = prediction['labels']\n",
    "    pred_scores = prediction['scores']\n",
    "    \n",
    "    # Find the index of the label with the highest probability\n",
    "    max_index = pred_scores.index(max(pred_scores))\n",
    "    \n",
    "    # Extract the label and its corresponding probability\n",
    "    max_label = pred_labels[max_index]\n",
    "    max_probability = pred_scores[max_index]\n",
    "    \n",
    "    return max_label, max_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trends_classify(filter, df_labels, df_gscore, classifier, limititer=0):\n",
    "    \"\"\"Function for labelling dataframe with labels and scores based on highest prediction probability\n",
    "    Args:\n",
    "        filter (str): filter string used for filtering dataframe by certain classification_product\n",
    "        df_labels (dataframe): dataframe df_labels containing all labels to be filtered by filter.\n",
    "        df_gscore (dataframe): dataframe df_gscore containing all articles with text to be classified, to be filtered by filter.\n",
    "        classifier (pipeline): classifier for labelling.\n",
    "        limititer (int): Defaults to 0. If int is given iteration over rows will stop at value of int\n",
    "    Returns:\n",
    "        df_gscore_iter: dataframe df_score with two new columns (label and proba) and several labelingresults per row\n",
    "    \"\"\"\n",
    "    iter = filter\n",
    "\n",
    "    df_labels_per_category = df_labels[df_labels['classification_product'] == iter]\n",
    "    candidate_labels = df_labels_per_category['query'].astype(str).tolist()\n",
    "\n",
    "    df_gscore_iter = df_gscore[df_gscore['classification_product'] == iter]\n",
    "\n",
    "    # shorten df to only return limited rows per function call\n",
    "    if limititer > 0 and limititer < len(df_gscore_iter):\n",
    "        df_gscore_iter = df_gscore_iter.iloc[0:limititer]\n",
    "\n",
    "    tqdm.pandas(desc=f\"Googel search related keyword classification for {iter}\")\n",
    "    # replace progress_apply with apply if you run this on google colab with T4 or on a GPU\n",
    "    df_gscore_iter['predicted_query_label'], df_gscore_iter['predicted_probability'] = zip(*df_gscore_iter['text_to_classify'].apply(lambda x: get_predictions_score(classifier(x, candidate_labels))))\n",
    "\n",
    "    return df_gscore_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialise the pipeline\n",
    "\n",
    "- The selected pipeline is a pre trained multi language classification model named \"joeddav/xlm-roberta-large-xnli\". It will be downloaded to a local cache folder of the machine where this notebook is processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:04:09.373541: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-25 15:04:09.373575: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at joeddav/xlm-roberta-large-xnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialisation of the pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6815, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "page_id                   0\n",
       "classification_product    0\n",
       "abstract                  7\n",
       "meta_description          0\n",
       "meta_title                0\n",
       "text_to_classify          0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "relevant_columns = ['page_id', 'classification_product', 'abstract', 'meta_description', 'meta_title' ]\n",
    "df_gscore = df[relevant_columns].copy()\n",
    "df_gscore['text_to_classify'] = df_gscore['abstract'].fillna('') + ' ' + df_gscore['meta_description'].fillna('') + ' ' + df_gscore['meta_title'].fillna('')\n",
    "class_product = df.classification_product.unique().tolist()\n",
    "\n",
    "display(df_gscore.shape)\n",
    "display(df_gscore.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Classify the dataset\n",
    "- recommended to do a testrun with a small number for limited iterations (e.g. 1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for E-Auto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/17 [00:15<04:01, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Auto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2/17 [00:30<03:44, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Zubehör\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:34<02:22, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Motorrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [00:49<02:39, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Energie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5/17 [01:02<02:26, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Verkehr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [01:05<01:40,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Wallbox/Laden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7/17 [01:20<01:50, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Solaranlagen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [01:24<01:19,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for E-Bike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9/17 [01:39<01:26, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Fahrrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10/17 [01:54<01:24, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for E-Scooter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 11/17 [02:10<01:19, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Solarspeicher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 12/17 [02:24<01:07, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Balkonkraftwerk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 13/17 [02:39<00:55, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Solargenerator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 14/17 [02:44<00:34, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for THG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 15/17 [02:46<00:16,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Wärmepumpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 16/17 [03:01<00:10, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googel search related keyword classification for Versicherung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [03:17<00:00, 11.64s/it]\n"
     ]
    }
   ],
   "source": [
    "iterations = 0  # set to small number for testrun, e.g. 1 or 2 (0 means unlimited).\n",
    "\n",
    "# create empty instance of target dataframe\n",
    "df_gscore_out = pd.DataFrame(columns=relevant_columns + ['text_to_classify', 'predicted_query_label', 'predicted_probability'])\n",
    "for cp in tqdm(class_product):\n",
    "    print(f\"Googel search related keyword classification for {cp}\")\n",
    "    df_gscore_classified = trends_classify(cp, df_labels, df_gscore, classifier, limititer=iterations)\n",
    "    df_gscore_out = pd.concat([df_gscore_out, df_gscore_classified], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Join score to dataset and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labels['predicted_query_label'] equals to df_labels['query']\n",
    "df_labels = df_labels.rename(columns={'query': 'predicted_query_label', 'value': 'query_score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join score in one step\n",
    "df_gscore_new = df_gscore_out.merge(df_labels, on=['classification_product', 'predicted_query_label'], how='left')\n",
    "df_gscore_new = df_gscore_new.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351 entries, 0 to 350\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   predicted_query_label   351 non-null    object\n",
      " 1   query_score             351 non-null    int64 \n",
      " 2   classification_product  351 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(17, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(17, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_labels.info())\n",
    "display(df_gscore_out.shape)\n",
    "display(df_gscore_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gscore_new.to_csv('../data/google_trends/data_trends_classified.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. postprocessing analysis \n",
    "- processing in google colab successful after aprox. 1 h 30 min\n",
    "- cell ouptup became too big due to extensive usage of progress bars\n",
    "- 34 duplicates are in dataset which have to be removed\n",
    "\n",
    "--> add steps to previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = pd.read_csv('../data/google_trends/data_trends_classified_orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6849, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6849 entries, 0 to 6848\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Unnamed: 0              6849 non-null   int64  \n",
      " 1   page_id                 6849 non-null   int64  \n",
      " 2   classification_product  6849 non-null   object \n",
      " 3   abstract                6842 non-null   object \n",
      " 4   meta_description        6849 non-null   object \n",
      " 5   meta_title              6849 non-null   object \n",
      " 6   text_to_classify        6849 non-null   object \n",
      " 7   predicted_query_label   6849 non-null   object \n",
      " 8   predicted_probability   6849 non-null   float64\n",
      " 9   query_score             6849 non-null   int64  \n",
      "dtypes: float64(1), int64(3), object(6)\n",
      "memory usage: 535.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_post.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = df_post.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6815, 9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first.to_csv('../data/google_trends/data_trends_classified.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d-drivers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
