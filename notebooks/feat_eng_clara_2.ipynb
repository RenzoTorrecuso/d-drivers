{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf = pd.read_csv('../data/full_data.csv')\n",
    "df_scrape = pd.read_csv('../data/full_scraped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version_id_ong</th>\n",
       "      <th>page_id</th>\n",
       "      <th>version_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1037_0</td>\n",
       "      <td>1037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1037_0</td>\n",
       "      <td>1037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1037_0</td>\n",
       "      <td>1037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1037_0</td>\n",
       "      <td>1037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1037_0</td>\n",
       "      <td>1037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  version_id_ong  page_id  version_id\n",
       "0         1037_0     1037           0\n",
       "1         1037_0     1037           0\n",
       "2         1037_0     1037           0\n",
       "3         1037_0     1037           0\n",
       "4         1037_0     1037           0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ongoing version_id that is unique for each version page_id combination\n",
    "df_perf['version_id_ong'] = df_perf['page_id'].astype(str) + '_' + df_perf['version_id'].astype(str)\n",
    "df_perf[['version_id_ong', 'page_id', 'version_id']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thomas:\n",
    "We have three groups which depend on different concatenated unique keys:\n",
    "\n",
    "#### Group 1: external_impressions and external_clicks:\n",
    "\n",
    "page_id\n",
    "date\n",
    "\n",
    "#### Group 2: video_play, page_impressions, clickouts:\n",
    "\n",
    "page_id\n",
    "date\n",
    "URL\n",
    "Author (edge case)\n",
    "\n",
    "#### Group 3: daily_likes, daily_dislikes:\n",
    "\n",
    "page_id\n",
    "date\n",
    "publishe_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 20)\n",
    "\n",
    "# Columns that differ on a daily basis and need to be aggregated with a certain rule\n",
    "col_agg_1 = ['external_clicks', 'external_impressions']\n",
    "col_agg_2 = ['video_play', 'page_impressions', 'clickouts']\n",
    "col_agg_3 = ['daily_likes', 'daily_dislikes']\n",
    "\n",
    "# Columns that don't need to be aggregated but are the same for each version\n",
    "all_columns = df_perf.columns.tolist()\n",
    "col = [c for c in all_columns if c not in col_agg_1 and c not in col_agg_2 and c not in col_agg_3]\n",
    "# this includes: ['old_index', 'page_id', 'date', 'url', 'version_id', 'publish_date', 'word_count', 'words_scraped', 'classification_product', 'classification_type', 'page_name', 'authors', 'author_scraped', 'title', 'h1', 'abstract', 'last_update', 'image_url', 'version_id_ong']\n",
    "\n",
    "# Aggregate by version for columns with simple duplicates\n",
    "df_agg = df_perf[col].groupby('version_id_ong').first()\n",
    "\n",
    "# Aggregate by version for columns which need to be aggregated with a certain rule\n",
    "#tbd\n",
    "\n",
    "#df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variables:\n",
    "Impressions, Clicks, CTR (click-through-rate). The latter we create in the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Click through rate based on external clicks and impressions\n",
    "df_perf['ctr'] = df_perf['external_clicks'] / df_perf['external_impressions'] *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features:\n",
    "Category, \n",
    "Image, \n",
    "H1, \n",
    "Abstract, \n",
    "URL, \n",
    "Title, \n",
    "Word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the last part of the URL to analyze it and inhibit duplicate data with classification_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract last part of URL and clean it\n",
    "def extract_last_part(url):\n",
    "    url_text = url.rsplit('/', 1)[-1]\n",
    "    cleaned_url = url_text.split('_')[0]\n",
    "    cleaned_url_list = cleaned_url.split('-')\n",
    "    return cleaned_url_list\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df_scrape['url_text'] = df_scrape['url'].apply(extract_last_part)\n",
    "\n",
    "# Sum up all list items per ongoing Version ID and merge with original df\n",
    "df_feat = pd.merge(df_scrape, df_scrape.groupby('page_id')['url_text'].apply(lambda x: list(set(sum(x, [])))).reset_index(name='merged_url'), on='page_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform media column\n",
    "def media_type(df, media_type):\n",
    "    if 'img-wrapper' in media_type or any(item in media_type for item in ['image-gallery', 'mb-lg-7', 'mb-8']):\n",
    "        return 'img'\n",
    "    elif any(item in media_type for item in ['mb-3', 'video-player', 'recobar']):\n",
    "        return 'video'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df_feat['media_type'] = df_scrape['media_type'].apply(lambda x: media_type(df_feat, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['page_id', 'url', 'h1', 'author', 'date', 'abstract',\n",
       "       'main_text_length', 'meta_title', 'meta_description', 'meta_image_url',\n",
       "       'media_type', 'page_img_size', 'url_text', 'merged_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title length\n",
    "df_feat['meta_title_len'] = df_feat['meta_title'].str.len()\n",
    "\n",
    "# Meta description length\n",
    "df_feat['meta_desc_len'] = df_feat['meta_description'].str.len()\n",
    "\n",
    "# H1 length\n",
    "df_feat['h1_len'] = df_feat['h1'].str.len()\n",
    "\n",
    "# Abstract length\n",
    "df_feat['abstract_len'] = df_feat['abstract'].str.len()\n",
    "\n",
    "# URL length\n",
    "df_feat['merged_url_len'] = df_feat['merged_url'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['page_id', 'url', 'date', 'version_id', 'publish_date',\n",
    "       'word_count', 'classification_product', 'classification_type', 'page_name', 'authors','title',\n",
    "       'daily_likes', 'daily_dislikes', 'video_play', 'page_impressions',\n",
    "       'clickouts', 'external_clicks', 'external_impressions','ctr']\n",
    "\n",
    "on = ['url','page_id']\n",
    "\n",
    "df_merge = pd.merge(df_perf[col],df_feat,how='left',on='page_id')\n",
    "df_merge.drop(['url_x'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the most important words from title, h1, abstract, url with NLP (e.g. TF-IDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>url</th>\n",
       "      <th>h1</th>\n",
       "      <th>abstract</th>\n",
       "      <th>meta_title</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>merged_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1012169</td>\n",
       "      <td>https://efahrer....</td>\n",
       "      <td>ZDF-Doku analysi...</td>\n",
       "      <td>Ein Kommentar vo...</td>\n",
       "      <td>ZDF-Doku analysi...</td>\n",
       "      <td>Ein Kommentar vo...</td>\n",
       "      <td>[argumente, und,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1017691</td>\n",
       "      <td>https://efahrer....</td>\n",
       "      <td>Des einen Freud,...</td>\n",
       "      <td>Einige Gemeinden...</td>\n",
       "      <td>Des einen Freud,...</td>\n",
       "      <td>Einige Gemeinden...</td>\n",
       "      <td>[freud, solar, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016983</td>\n",
       "      <td>https://efahrer....</td>\n",
       "      <td>160 Kilometer mi...</td>\n",
       "      <td>Als Journalist u...</td>\n",
       "      <td>160 Kilometer mi...</td>\n",
       "      <td>Als Journalist u...</td>\n",
       "      <td>[dem, bike, pend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101399</td>\n",
       "      <td>https://efahrer....</td>\n",
       "      <td>Laden eines Elek...</td>\n",
       "      <td>Wer sein Elektro...</td>\n",
       "      <td>Ladestation für ...</td>\n",
       "      <td>Wer sein Elektro...</td>\n",
       "      <td>[hause, laden, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104461</td>\n",
       "      <td>https://efahrer....</td>\n",
       "      <td>Leasing-Deal für...</td>\n",
       "      <td>Den Mazda MX-30 ...</td>\n",
       "      <td>Leasing-Deal für...</td>\n",
       "      <td>Den Mazda MX-30 ...</td>\n",
       "      <td>[leasing, 150, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id                  url                   h1             abstract   \n",
       "0  1012169  https://efahrer....  ZDF-Doku analysi...  Ein Kommentar vo...  \\\n",
       "1  1017691  https://efahrer....  Des einen Freud,...  Einige Gemeinden...   \n",
       "2  1016983  https://efahrer....  160 Kilometer mi...  Als Journalist u...   \n",
       "3   101399  https://efahrer....  Laden eines Elek...  Wer sein Elektro...   \n",
       "4   104461  https://efahrer....  Leasing-Deal für...  Den Mazda MX-30 ...   \n",
       "\n",
       "            meta_title     meta_description           merged_url  \n",
       "0  ZDF-Doku analysi...  Ein Kommentar vo...  [argumente, und,...  \n",
       "1  Des einen Freud,...  Einige Gemeinden...  [freud, solar, d...  \n",
       "2  160 Kilometer mi...  Als Journalist u...  [dem, bike, pend...  \n",
       "3  Ladestation für ...  Wer sein Elektro...  [hause, laden, a...  \n",
       "4  Leasing-Deal für...  Den Mazda MX-30 ...  [leasing, 150, s...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nlp = df_feat.drop(['author','date','meta_image_url','media_type','page_img_size','url_text','meta_title_len','meta_desc_len', 'h1_len','abstract_len', 'merged_url_len'],axis=1)\n",
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/clara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/clara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/clara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):        \n",
    "        words = word_tokenize(text)\n",
    "        # Remove punctuation and special characters\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Remove stopwords\n",
    "        return ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords_from_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(remove_stopwords)\n",
    "    return df\n",
    "\n",
    "columns_to_clean = ['h1','abstract','meta_title','meta_description','merged_url']\n",
    "df_nlp = remove_stopwords_from_columns(df_nlp, columns_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.865000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.320724e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.613835e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.011990e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014101e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.016311e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.018782e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            page_id\n",
       "count  6.865000e+03\n",
       "mean   9.320724e+05\n",
       "std    2.613835e+05\n",
       "min    3.000000e+01\n",
       "25%    1.011990e+06\n",
       "50%    1.014101e+06\n",
       "75%    1.016311e+06\n",
       "max    1.018782e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nlp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_vec = df_nlp.copy()\n",
    "df_nlp_vec.drop(['url','page_id'],axis=1,inplace=True)\n",
    "df_nlp_vec.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h1                  0\n",
       "abstract            0\n",
       "meta_title          0\n",
       "meta_description    0\n",
       "merged_url          0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nlp_vec.isna().sum() #info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(column, df):\n",
    "    col = df[column]\n",
    "    vect = CountVectorizer().fit(col)\n",
    "    transformed = vect.transform(col)\n",
    "    \n",
    "    # Create a DataFrame from the transformed array\n",
    "    df_transformed = pd.DataFrame(transformed.toarray(), columns=vect.get_feature_names_out(), index=df.index)\n",
    "    \n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    df_concatenated = pd.concat([df, df_transformed], axis=1)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df_concatenated.drop(columns=[column], inplace=True)\n",
    "    \n",
    "    return df_concatenated\n",
    "\n",
    "# Iterate over columns to clean\n",
    "for item in columns_to_clean:\n",
    "    df_nlp_vec = vectorize_text(column=item, df=df_nlp_vec)\n",
    "    filename = f'../data/nlp_features_{item}.csv'\n",
    "    df_nlp_vec.to_csv(filename, encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
